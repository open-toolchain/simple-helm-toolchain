---
defaultBaseImageVersion: latest
properties:
- name: HELM_VERSION
  value: ''
  type: text
stages:
- name: BUILD
  inputs:
  - type: git
    branch: master
    service: ${GIT_REPO}    
  triggers:
  - type: commit
  properties:
  - name: DOCKER_ROOT
    value: .
    type: text
  - name: DOCKER_FILE
    value: Dockerfile
    type: text  
  jobs:
  - name: Pre-build check
    type: builder
    build_type: cr
    artifact_dir: ''
    target:
      region_id: ${REGISTRY_REGION_ID}
      api_key: ${API_KEY}
    namespace: ${REGISTRY_NAMESPACE}
    image_name: ${CF_APP_NAME}
    script: |-
      #!/bin/bash
      # uncomment to debug the script
      # set -x
      # copy the script below into your app code repo (e.g. ./scripts/check_prebuild.sh) and 'source' it from your pipeline job
      #    source ./scripts/check_prebuild.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_prebuild.sh")
      # ------------------
      # source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_prebuild.sh
      ibmcloud plugin update container-registry
      echo "REGISTRY_URL=${REGISTRY_URL}"
      echo "REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE}"
      echo "IMAGE_NAME=${IMAGE_NAME}"
      echo "ARCHIVE_DIR=${ARCHIVE_DIR}"
      # View build properties
      if [ -f build.properties ]; then 
        echo "build.properties:"
        cat build.properties
      else 
        echo "build.properties : not found"
      fi
      # also run 'env' command to find all available env variables
      # or learn more about the available environment variables at:
      # https://cloud.ibm.com/docs/services/ContinuousDelivery/pipeline_deploy_var.html#deliverypipeline_environment
      if [ -z "${DOCKER_ROOT}" ]; then DOCKER_ROOT=. ; fi
      if [ -z "${DOCKER_FILE}" ]; then DOCKER_FILE=${DOCKER_ROOT}/Dockerfile ; fi
      echo "=========================================================="
      echo "Checking for Dockerfile at the repository root"
      if [ -f "${DOCKER_FILE}" ]; then 
      echo -e "Dockerfile found at: ${DOCKER_FILE}"
      else
          echo "Dockerfile not found at: ${DOCKER_FILE}"
          exit 1
      fi
      echo "Linting Dockerfile"
      npm install -g dockerlint
      dockerlint -f Dockerfile
      echo "=========================================================="
      echo "Checking registry current plan and quota"
      ibmcloud cr plan
      ibmcloud cr quota
      echo "If needed, discard older images using: ibmcloud cr image-untag"
      echo "Checking registry namespace: ${REGISTRY_NAMESPACE}"
      NS=$( ibmcloud cr namespaces | grep ${REGISTRY_NAMESPACE} ||: )
      if [ -z "${NS}" ]; then
          echo "Registry namespace ${REGISTRY_NAMESPACE} not found, creating it."
          ibmcloud cr namespace-add ${REGISTRY_NAMESPACE}
          echo "Registry namespace ${REGISTRY_NAMESPACE} created."
      else 
          echo "Registry namespace ${REGISTRY_NAMESPACE} found."
      fi
      echo -e "Existing images in registry..."
      ibmcloud cr image-list --restrict ${REGISTRY_NAMESPACE}
      echo "=========================================================="
      KEEP=1
      COUNT=0
      echo -e "PURGING REGISTRY: Keeping last ${KEEP} build image(s) based on <Repository:Tag>..."
      LIST=$( ibmcloud cr image-list --restrict ${REGISTRY_NAMESPACE}/${IMAGE_NAME} --no-trunc --format '{{ .Created }} {{ .Repository }}:{{ .Tag}}' | sort -r -u | awk '{print $2}' )
      while IFS= read -r IMAGE_URL ; do
        if [[ ! -z "$IMAGE_URL" ]]; then
          if [[ $COUNT -lt $KEEP ]]; then
            echo "Keeping build image: ${IMAGE_URL}..."
          else
            echo "Purging build image: ${IMAGE_URL}"
            # Refer to: https://cloud.ibm.com/docs/container-registry-cli-plugin?topic=container-registry-cli-plugin-containerregcli#bx_cr_image_untag
            ibmcloud cr image-untag "${IMAGE_URL}"
          fi
        fi
        COUNT=$(( COUNT + 1 ))
      done <<< "$LIST"
      echo "Contents of build image registry:"
      ibmcloud cr image-list --restrict ${REGISTRY_NAMESPACE}

  - name: Build Docker image
    type: builder
    build_type: cr
    artifact_dir: output
    target:
      region_id: ${REGISTRY_REGION_ID}
      api_key: ${API_KEY}
    namespace: ${REGISTRY_NAMESPACE}
    image_name: ${CF_APP_NAME}
    script: |
      #!/bin/bash
      # uncomment to debug the script
      #set -x
      # copy the script below into your app code repo (e.g. ./scripts/build_image.sh) and 'source' it from your pipeline job
      #    source ./scripts/build_image.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/build_image.sh")
      # ------------------
      # source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/build_image.sh

      # This script does build a Docker image into IBM Cloud kubernetes service private image registry, and copies information into
      # a build.properties file, so they can be reused later on by other scripts (e.g. image url, chart name, ...)
      echo "REGISTRY_URL=${REGISTRY_URL}"
      echo "REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE}"
      echo "IMAGE_NAME=${IMAGE_NAME}"
      echo "BUILD_NUMBER=${BUILD_NUMBER}"
      echo "ARCHIVE_DIR=${ARCHIVE_DIR}"
      echo "GIT_BRANCH=${GIT_BRANCH}"
      echo "GIT_COMMIT=${GIT_COMMIT}"
      echo "DOCKER_ROOT=${DOCKER_ROOT}"
      echo "DOCKER_FILE=${DOCKER_FILE}"

      # View build properties
      if [ -f build.properties ]; then 
        echo "build.properties:"
        cat build.properties
      else 
        echo "build.properties : not found"
      fi 
      # also run 'env' command to find all available env variables
      # or learn more about the available environment variables at:
      # https://cloud.ibm.com/docs/services/ContinuousDelivery/pipeline_deploy_var.html#deliverypipeline_environment

      # To review or change build options use:
      # ibmcloud cr build --help

      echo -e "Existing images in registry"
      ibmcloud cr images

      # Minting image tag using format: BUILD_NUMBER--BRANCH-COMMIT_ID-TIMESTAMP
      # e.g. 3-master-50da6912-20181123114435
      # (use build number as first segment to allow image tag as a patch release name according to semantic versioning)

      TIMESTAMP=$( date -u "+%Y%m%d%H%M%S")
      IMAGE_TAG=${TIMESTAMP}
      if [ ! -z "${GIT_COMMIT}" ]; then
        GIT_COMMIT_SHORT=$( echo ${GIT_COMMIT} | head -c 8 ) 
        IMAGE_TAG=${GIT_COMMIT_SHORT}-${IMAGE_TAG}
      fi
      if [ ! -z "${GIT_BRANCH}" ]; then IMAGE_TAG=${GIT_BRANCH}-${IMAGE_TAG} ; fi
      IMAGE_TAG=${BUILD_NUMBER}-${IMAGE_TAG}
      echo "=========================================================="
      echo -e "BUILDING CONTAINER IMAGE: ${IMAGE_NAME}:${IMAGE_TAG}"
      if [ -z "${DOCKER_ROOT}" ]; then DOCKER_ROOT=. ; fi
      if [ -z "${DOCKER_FILE}" ]; then DOCKER_FILE=${DOCKER_ROOT}/Dockerfile ; fi
      set -x
      ibmcloud cr build -t ${REGISTRY_URL}/${REGISTRY_NAMESPACE}/${IMAGE_NAME}:${IMAGE_TAG} ${DOCKER_ROOT} -f ${DOCKER_FILE}
      set +x

      ibmcloud cr image-inspect ${REGISTRY_URL}/${REGISTRY_NAMESPACE}/${IMAGE_NAME}:${IMAGE_TAG}

      # Set PIPELINE_IMAGE_URL for subsequent jobs in stage (e.g. Vulnerability Advisor)
      export PIPELINE_IMAGE_URL="$REGISTRY_URL/$REGISTRY_NAMESPACE/$IMAGE_NAME:$IMAGE_TAG"

      ibmcloud cr images --restrict ${REGISTRY_NAMESPACE}/${IMAGE_NAME}

      ######################################################################################
      # Copy any artifacts that will be needed for deployment and testing to $WORKSPACE    #
      ######################################################################################
      echo "=========================================================="
      echo "COPYING ARTIFACTS needed for deployment and testing (in particular build.properties)"

      echo "Checking archive dir presence"
      if [ -z "${ARCHIVE_DIR}" ]; then
        echo -e "Build archive directory contains entire working directory."
      else
        echo -e "Copying working dir into build archive directory: ${ARCHIVE_DIR} "
        mkdir -p ${ARCHIVE_DIR}
        find . -mindepth 1 -maxdepth 1 -not -path "./$ARCHIVE_DIR" -exec cp -R '{}' "${ARCHIVE_DIR}/" ';'
      fi

      # Persist env variables into a properties file (build.properties) so that all pipeline stages consuming this
      # build as input and configured with an environment properties file valued 'build.properties'
      # will be able to reuse the env variables in their job shell scripts.

      # If already defined build.properties from prior build job, append to it.
      cp build.properties $ARCHIVE_DIR/ || :

      # IMAGE information from build.properties is used in Helm Chart deployment to set the release name
      echo "IMAGE_NAME=${IMAGE_NAME}" >> $ARCHIVE_DIR/build.properties
      echo "IMAGE_TAG=${IMAGE_TAG}" >> $ARCHIVE_DIR/build.properties
      # REGISTRY information from build.properties is used in Helm Chart deployment to generate cluster secret
      echo "REGISTRY_URL=${REGISTRY_URL}" >> $ARCHIVE_DIR/build.properties
      echo "REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE}" >> $ARCHIVE_DIR/build.properties
      echo "GIT_BRANCH=${GIT_BRANCH}" >> $ARCHIVE_DIR/build.properties
      echo "File 'build.properties' created for passing env variables to subsequent pipeline jobs:"
      cat $ARCHIVE_DIR/build.properties
- name: VALIDATE
  inputs:
  - type: job
    stage: BUILD
    job: Build Docker image
  triggers:
  - type: stage
  properties:
  - name: buildprops
    value: build.properties
    type: file
  jobs:
  - name: Vulnerability Advisor
    type: tester
    test_type: vulnerabilityadvisor
    use_image_from_build_input: true
    fail_stage: false
    target:
      region_id: ${REGISTRY_REGION_ID}
      api_key: ${API_KEY}
    script: |
      #!/bin/bash
      # uncomment to debug the script
      # set -x
      # copy the script below into your app code repo (e.g. ./scripts/check_vulnerabilities.sh) and 'source' it from your pipeline job
      #    source ./scripts/check_vulnerabilities.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_vulnerabilities.sh")
      # ------------------
      # source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_vulnerabilities.sh
      # Input env variables (can be received via a pipeline environment properties.file.

      # View build properties
      if [ -f build.properties ]; then 
        echo "build.properties:"
        cat build.properties
      else 
        echo "build.properties : not found"
      fi 

      # If running after build_image.sh in same stage, reuse the exported variable PIPELINE_IMAGE_URL
      if [ -z "${PIPELINE_IMAGE_URL}" ]; then
        PIPELINE_IMAGE_URL=${REGISTRY_URL}/${REGISTRY_NAMESPACE}/${IMAGE_NAME}:${IMAGE_TAG}
      else
        # extract from img url
        REGISTRY_URL=$(echo ${PIPELINE_IMAGE_URL} | cut -f1 -d/)
        REGISTRY_NAMESPACE=$(echo ${PIPELINE_IMAGE_URL} | cut -f2 -d/)
        IMAGE_NAME=$(echo ${PIPELINE_IMAGE_URL} | cut -f3 -d/ | cut -f1 -d:)
        IMAGE_TAG=$(echo ${PIPELINE_IMAGE_URL} | cut -f3 -d/ | cut -f2 -d:)
      fi
      echo "PIPELINE_IMAGE_URL=${PIPELINE_IMAGE_URL}"
      echo "REGISTRY_URL=${REGISTRY_URL}"
      echo "REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE}"
      echo "IMAGE_NAME=${IMAGE_NAME}"
      echo "IMAGE_TAG=${IMAGE_TAG}"

      # also run 'env' command to find all available env variables
      # or learn more about the available environment variables at:
      # https://cloud.ibm.com/docs/services/ContinuousDelivery/pipeline_deploy_var.html#deliverypipeline_environment

      ibmcloud cr images --restrict ${REGISTRY_NAMESPACE}/${IMAGE_NAME}
      echo -e "Checking vulnerabilities in image: ${PIPELINE_IMAGE_URL}"
      for ITER in {1..30}
      do
        set +e
        STATUS=$( ibmcloud cr va -e -o json ${PIPELINE_IMAGE_URL} | jq -r '.[0].status' )
        set -e
        # Possible status from Vulnerability Advisor: OK, UNSUPPORTED, INCOMPLETE, UNSCANNED, FAIL, WARN
        if [[ ${STATUS} != "INCOMPLETE" && ${STATUS} != "UNSCANNED" ]]; then
          break
        fi
        echo -e "${ITER} STATUS ${STATUS} : A vulnerability report was not found for the specified image."
        echo "Either the image doesn't exist or the scan hasn't completed yet. "
        echo "Waiting for scan to complete..."
        sleep 10
      done
      set +e
      ibmcloud cr va -e ${PIPELINE_IMAGE_URL}
      set -e
      STATUS=$( ibmcloud cr va -e -o json ${PIPELINE_IMAGE_URL} | jq -r '.[0].status' )
      [[ ${STATUS} == "OK" ]] || [[ ${STATUS} == "UNSUPPORTED" ]] || [[ ${STATUS} == "WARN" ]] || { echo "ERROR: The vulnerability scan was not successful, check the OUTPUT of the command and try again."; exit 1; }
- name: PROD
  inputs:
  - type: job
    stage: BUILD
    job: Build Docker image
  triggers:
  - type: stage
  properties:
  - name: buildprops
    value: build.properties
    type: file     
  - name: CLUSTER_NAMESPACE
    value: ${PROD_CLUSTER_NAMESPACE}
    type: text
  - name: CHART_ROOT
    value: chart
    type: text      
  jobs:
  - name: Pre-deploy check
    type: deployer
    target:
      region_id: ${PROD_REGION_ID}
      resource_group: ${PROD_RESOURCE_GROUP}
      api_key: ${API_KEY}
      kubernetes_cluster: ${PROD_CLUSTER_NAME}
    script: |
      #!/bin/bash
      # uncomment to debug the script
      #set -x
      # copy the script below into your app code repo (e.g. ./scripts/check_predeploy_helm.sh) and 'source' it from your pipeline job
      #    source ./scripts/check_predeploy_helm.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_predeploy_helm.sh")
      # ------------------
      # source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_predeploy_helm.sh

      # This script checks the IBM Container Service cluster is ready, has a namespace configured with access to the private
      # image registry (using an IBM Cloud API Key). It also configures Helm Tiller service to later perform a deploy with Helm.

      # Input env variables (can be received via a pipeline environment properties.file.
      echo "IMAGE_NAME=${IMAGE_NAME}"
      echo "IMAGE_TAG=${IMAGE_TAG}"
      echo "CHART_ROOT=${CHART_ROOT}"
      echo "REGISTRY_URL=${REGISTRY_URL}"
      echo "REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE}"
      echo "HELM_VERSION=${HELM_VERSION}"

      # View build properties
      if [ -f build.properties ]; then 
        echo "build.properties:"
        cat build.properties
      else 
        echo "build.properties : not found"
      fi 
      # also run 'env' command to find all available env variables
      # or learn more about the available environment variables at:
      # https://cloud.ibm.com/docs/services/ContinuousDelivery/pipeline_deploy_var.html#deliverypipeline_environment

      # Input env variables from pipeline job
      echo "PIPELINE_KUBERNETES_CLUSTER_NAME=${PIPELINE_KUBERNETES_CLUSTER_NAME}"
      echo "CLUSTER_NAMESPACE=${CLUSTER_NAMESPACE}"

      echo "=========================================================="
      echo "CHECKING HELM CHART"
      if [ -z "${CHART_ROOT}" ]; then CHART_ROOT="chart" ; fi
      echo -e "Looking for chart under /${CHART_ROOT}/<CHART_NAME>"
      if [ -d ${CHART_ROOT} ]; then
        CHART_NAME=$(find ${CHART_ROOT}/. -maxdepth 2 -type d -name '[^.]?*' -printf %f -quit)
        CHART_PATH=${CHART_ROOT}/${CHART_NAME}
      fi
      if [ -z "${CHART_PATH}" ]; then
          echo -e "No Helm chart found for Kubernetes deployment under ${CHART_ROOT}/<CHART_NAME>."
          exit 1
      else
          echo -e "Helm chart found for Kubernetes deployment : ${CHART_PATH}"
      fi
      echo "Linting Helm Chart"
      helm lint ${CHART_PATH}

      #Check cluster availability
      echo "=========================================================="
      echo "CHECKING CLUSTER readiness and namespace existence"
      IP_ADDR=$( ibmcloud cs workers ${PIPELINE_KUBERNETES_CLUSTER_NAME} | grep normal | awk '{ print $2 }' )
      if [ -z "${IP_ADDR}" ]; then
        echo -e "${PIPELINE_KUBERNETES_CLUSTER_NAME} not created or workers not ready"
        exit 1
      fi
      echo "Configuring cluster namespace"
      if kubectl get namespace ${CLUSTER_NAMESPACE}; then
        echo -e "Namespace ${CLUSTER_NAMESPACE} found."
      else
        kubectl create namespace ${CLUSTER_NAMESPACE}
        echo -e "Namespace ${CLUSTER_NAMESPACE} created."
      fi

      # Grant access to private image registry from namespace $CLUSTER_NAMESPACE
      # reference https://cloud.ibm.com/docs/containers/cs_cluster.html#bx_registry_other
      echo "=========================================================="
      echo -e "CONFIGURING ACCESS to private image registry from namespace ${CLUSTER_NAMESPACE}"
      IMAGE_PULL_SECRET_NAME="ibmcloud-toolchain-${PIPELINE_TOOLCHAIN_ID}-${REGISTRY_URL}"

      echo -e "Checking for presence of ${IMAGE_PULL_SECRET_NAME} imagePullSecret for this toolchain"
      if ! kubectl get secret ${IMAGE_PULL_SECRET_NAME} --namespace ${CLUSTER_NAMESPACE}; then
        echo -e "${IMAGE_PULL_SECRET_NAME} not found in ${CLUSTER_NAMESPACE}, creating it"
        # for Container Registry, docker username is 'token' and email does not matter
        kubectl --namespace ${CLUSTER_NAMESPACE} create secret docker-registry ${IMAGE_PULL_SECRET_NAME} --docker-server=${REGISTRY_URL} --docker-password=${PIPELINE_BLUEMIX_API_KEY} --docker-username=iamapikey --docker-email=a@b.com
      else
        echo -e "Namespace ${CLUSTER_NAMESPACE} already has an imagePullSecret for this toolchain."
      fi
      echo "Checking ability to pass pull secret via Helm chart (see also https://cloud.ibm.com/docs/containers/cs_images.html#images)"
      CHART_PULL_SECRET=$( grep 'pullSecret' ${CHART_PATH}/values.yaml || : )
      if [ -z "${CHART_PULL_SECRET}" ]; then
        echo "INFO: Chart is not expecting an explicit private registry imagePullSecret. Patching the cluster default serviceAccount to pass it implicitly instead."
        echo "      Learn how to inject pull secrets into the deployment chart at: https://kubernetes.io/docs/concepts/containers/images/#referring-to-an-imagepullsecrets-on-a-pod"
        echo "      or check out this chart example: https://github.com/open-toolchain/hello-helm/tree/master/chart/hello"
        SERVICE_ACCOUNT=$(kubectl get serviceaccount default  -o json --namespace ${CLUSTER_NAMESPACE} )
        if ! echo ${SERVICE_ACCOUNT} | jq -e '. | has("imagePullSecrets")' > /dev/null ; then
          kubectl patch --namespace ${CLUSTER_NAMESPACE} serviceaccount/default -p '{"imagePullSecrets":[{"name":"'"${IMAGE_PULL_SECRET_NAME}"'"}]}'
        else
          if echo ${SERVICE_ACCOUNT} | jq -e '.imagePullSecrets[] | select(.name=="'"${IMAGE_PULL_SECRET_NAME}"'")' > /dev/null ; then 
            echo -e "Pull secret already found in default serviceAccount"
          else
            echo "Inserting toolchain pull secret into default serviceAccount"
            kubectl patch --namespace ${CLUSTER_NAMESPACE} serviceaccount/default --type='json' -p='[{"op":"add","path":"/imagePullSecrets/-","value":{"name": "'"${IMAGE_PULL_SECRET_NAME}"'"}}]'
          fi
        fi
        echo "default serviceAccount:"
        kubectl get serviceaccount default --namespace ${CLUSTER_NAMESPACE} -o yaml
        echo -e "Namespace ${CLUSTER_NAMESPACE} authorizing with private image registry using patched default serviceAccount"
      else
        echo -e "Namespace ${CLUSTER_NAMESPACE} authorized with private image registry using Helm chart imagePullSecret"
      fi

      echo "=========================================================="
      echo "CHECKING HELM VERSION: matching Helm Tiller (server) if detected. "
      set +e
      LOCAL_VERSION=$( helm version --client | grep SemVer: | sed "s/^.*SemVer:\"v\([0-9.]*\).*/\1/" )
      TILLER_VERSION=$( helm version --server | grep SemVer: | sed "s/^.*SemVer:\"v\([0-9.]*\).*/\1/" )
      set -e
      if [ -z "${TILLER_VERSION}" ]; then
        if [ -z "${HELM_VERSION}" ]; then
          CLIENT_VERSION=${LOCAL_VERSION}
        else
          CLIENT_VERSION=${HELM_VERSION}
        fi
      else
        echo -e "Helm Tiller ${TILLER_VERSION} already installed in cluster. Keeping it, and aligning client."
        CLIENT_VERSION=${TILLER_VERSION}
      fi
      if [ "${CLIENT_VERSION}" != "${LOCAL_VERSION}" ]; then
        echo -e "Installing Helm client ${CLIENT_VERSION}"
        WORKING_DIR=$(pwd)
        mkdir ~/tmpbin && cd ~/tmpbin
        curl -L https://storage.googleapis.com/kubernetes-helm/helm-v${CLIENT_VERSION}-linux-amd64.tar.gz -o helm.tar.gz && tar -xzvf helm.tar.gz
        cd linux-amd64
        export PATH=$(pwd):$PATH
        cd $WORKING_DIR
      fi
      if [ -z "${TILLER_VERSION}" ]; then
          echo -e "Installing Helm Tiller ${CLIENT_VERSION} with cluster admin privileges (RBAC)"
          kubectl -n kube-system create serviceaccount tiller
          kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
          helm init --service-account tiller
          # helm init --upgrade --force-upgrade
          kubectl --namespace=kube-system rollout status deploy/tiller-deploy
          # kubectl rollout status -w deployment/tiller-deploy --namespace=kube-system
      fi
      helm version

      echo "=========================================================="
      echo -e "CHECKING HELM releases in this namespace: ${CLUSTER_NAMESPACE}"
      helm list --namespace ${CLUSTER_NAMESPACE}
  - name: Deploy Helm chart
    type: deployer
    target:
      region_id: ${PROD_REGION_ID}
      resource_group: ${PROD_RESOURCE_GROUP}
      api_key: ${API_KEY}
      kubernetes_cluster: ${PROD_CLUSTER_NAME}
    script: |
      #!/bin/bash
      # uncomment to debug the script
      #set -x
      # copy the script below into your app code repo (e.g. ./scripts/deploy_helm.sh) and 'source' it from your pipeline job
      #    source ./scripts/deploy_helm.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/deploy_helm.sh")
      # ------------------
      # source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/deploy_helm.sh
      # Input env variables (can be received via a pipeline environment properties.file.
      echo "IMAGE_NAME=${IMAGE_NAME}"
      echo "IMAGE_TAG=${IMAGE_TAG}"
      echo "CHART_ROOT=${CHART_ROOT}"
      echo "REGISTRY_URL=${REGISTRY_URL}"
      echo "REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE}"
      echo "CLUSTER_NAMESPACE=${CLUSTER_NAMESPACE}"
      echo "USE_ISTIO_GATEWAY=${USE_ISTIO_GATEWAY}"
      echo "HELM_VERSION=${HELM_VERSION}"

      # View build properties
      if [ -f build.properties ]; then 
        echo "build.properties:"
        cat build.properties
      else 
        echo "build.properties : not found"
      fi 

      # also run 'env' command to find all available env variables
      # or learn more about the available environment variables at:
      # https://cloud.ibm.com/docs/services/ContinuousDelivery/pipeline_deploy_var.html#deliverypipeline_environment

      # Input env variables from pipeline job
      echo "PIPELINE_KUBERNETES_CLUSTER_NAME=${PIPELINE_KUBERNETES_CLUSTER_NAME}"
      if [ -z "${CLUSTER_NAMESPACE}" ]; then CLUSTER_NAMESPACE=default ; fi
      echo "CLUSTER_NAMESPACE=${CLUSTER_NAMESPACE}"

      echo "=========================================================="
      echo "CHECKING HELM CHART"
      if [ -z "${CHART_ROOT}" ]; then CHART_ROOT="chart" ; fi
      echo -e "Looking for chart under /${CHART_ROOT}/<CHART_NAME>"
      if [ -d ${CHART_ROOT} ]; then
        CHART_NAME=$(find ${CHART_ROOT}/. -maxdepth 2 -type d -name '[^.]?*' -printf %f -quit)
        CHART_PATH=${CHART_ROOT}/${CHART_NAME}
      fi
      if [ -z "${CHART_PATH}" ]; then
          echo -e "No Helm chart found for Kubernetes deployment under ${CHART_ROOT}/<CHART_NAME>."
          exit 1
      else
          echo -e "Helm chart found for Kubernetes deployment : ${CHART_PATH}"
      fi

      echo "=========================================================="
      echo "DEFINE RELEASE by prefixing image (app) name with namespace if not 'default' as Helm needs unique release names across namespaces"
      if [[ "${CLUSTER_NAMESPACE}" != "default" ]]; then
        RELEASE_NAME="${CLUSTER_NAMESPACE}-${IMAGE_NAME}"
      else
        RELEASE_NAME=${IMAGE_NAME}
      fi
      echo -e "Release name: ${RELEASE_NAME}"

      echo "=========================================================="
      echo "CHECKING HELM CLIENT VERSION: matching Helm Tiller (server) if detected. "
      set +e
      LOCAL_VERSION=$( helm version --client | grep SemVer: | sed "s/^.*SemVer:\"v\([0-9.]*\).*/\1/" )
      TILLER_VERSION=$( helm version --server | grep SemVer: | sed "s/^.*SemVer:\"v\([0-9.]*\).*/\1/" )
      set -e
      if [ -z "${TILLER_VERSION}" ]; then
        if [ -z "${HELM_VERSION}" ]; then
          CLIENT_VERSION=${LOCAL_VERSION}
        else
          CLIENT_VERSION=${HELM_VERSION}
        fi
      else
        echo -e "Helm Tiller ${TILLER_VERSION} already installed in cluster. Keeping it, and aligning client."
        CLIENT_VERSION=${TILLER_VERSION}
      fi
      if [ "${CLIENT_VERSION}" != "${LOCAL_VERSION}" ]; then
        echo -e "Installing Helm client ${CLIENT_VERSION}"
        WORKING_DIR=$(pwd)
        mkdir ~/tmpbin && cd ~/tmpbin
        curl -L https://storage.googleapis.com/kubernetes-helm/helm-v${CLIENT_VERSION}-linux-amd64.tar.gz -o helm.tar.gz && tar -xzvf helm.tar.gz
        cd linux-amd64
        export PATH=$(pwd):$PATH
        cd $WORKING_DIR
      fi
      helm version --client

      echo "=========================================================="
      echo "DEPLOYING HELM chart"
      IMAGE_REPOSITORY=${REGISTRY_URL}/${REGISTRY_NAMESPACE}/${IMAGE_NAME}
      IMAGE_PULL_SECRET_NAME="ibmcloud-toolchain-${PIPELINE_TOOLCHAIN_ID}-${REGISTRY_URL}"

      # Using 'upgrade --install" for rolling updates. Note that subsequent updates will occur in the same namespace the release is currently deployed in, ignoring the explicit--namespace argument".
      echo -e "Dry run into: ${PIPELINE_KUBERNETES_CLUSTER_NAME}/${CLUSTER_NAMESPACE}."
      helm upgrade --install --debug --dry-run ${RELEASE_NAME} ${CHART_PATH} --set image.repository=${IMAGE_REPOSITORY},image.tag=${IMAGE_TAG},image.pullSecret=${IMAGE_PULL_SECRET_NAME} --namespace ${CLUSTER_NAMESPACE}

      echo -e "Deploying into: ${PIPELINE_KUBERNETES_CLUSTER_NAME}/${CLUSTER_NAMESPACE}."
      helm upgrade  --install ${RELEASE_NAME} ${CHART_PATH} --set image.repository=${IMAGE_REPOSITORY},image.tag=${IMAGE_TAG},image.pullSecret=${IMAGE_PULL_SECRET_NAME} --namespace ${CLUSTER_NAMESPACE}

      echo "=========================================================="
      echo -e "CHECKING deployment status of release ${RELEASE_NAME} with image tag: ${IMAGE_TAG}"
      echo ""
      for ITERATION in {1..30}
      do
        DATA=$( kubectl get pods --namespace ${CLUSTER_NAMESPACE} -o json )
        NOT_READY=$( echo $DATA | jq '.items[].status | select(.containerStatuses!=null) | .containerStatuses[] | select(.image=="'"${IMAGE_REPOSITORY}:${IMAGE_TAG}"'") | select(.ready==false) ' )
        if [[ -z "$NOT_READY" ]]; then
          echo -e "All pods are ready:"
          echo $DATA | jq '.items[].status | select(.containerStatuses!=null) | .containerStatuses[] | select(.image=="'"${IMAGE_REPOSITORY}:${IMAGE_TAG}"'") | select(.ready==true) '
          break # deployment succeeded
        fi
        REASON=$(echo $DATA | jq '.items[].status | select(.containerStatuses!=null) | .containerStatuses[] | select(.image=="'"${IMAGE_REPOSITORY}:${IMAGE_TAG}"'") | .state.waiting.reason')
        echo -e "${ITERATION} : Deployment still pending..."
        echo -e "NOT_READY:${NOT_READY}"
        echo -e "REASON: ${REASON}"
        if [[ ${REASON} == *ErrImagePull* ]] || [[ ${REASON} == *ImagePullBackOff* ]]; then
          echo "Detected ErrImagePull or ImagePullBackOff failure. "
          echo "Please check image still exists in registry, and proper permissions from cluster to image registry (e.g. image pull secret)"
          break; # no need to wait longer, error is fatal
        elif [[ ${REASON} == *CrashLoopBackOff* ]]; then
          echo "Detected CrashLoopBackOff failure. "
          echo "Application is unable to start, check the application startup logs"
          break; # no need to wait longer, error is fatal
        fi
        sleep 5
      done

      if [[ ! -z "$NOT_READY" ]]; then
        echo ""
        echo "=========================================================="
        echo "DEPLOYMENT FAILED"
        echo "Deployed Services:"
        kubectl describe services --namespace ${CLUSTER_NAMESPACE}
        echo ""
        echo "Deployed Pods:"
        kubectl describe pods --namespace ${CLUSTER_NAMESPACE}
        echo ""
        #echo "Application Logs"
        #kubectl logs --selector app=${CHART_NAME} --namespace ${CLUSTER_NAMESPACE}
        echo "=========================================================="
        PREVIOUS_RELEASE=$( helm history ${RELEASE_NAME} | grep SUPERSEDED | sort -r -n | awk '{print $1}' | head -n 1 )
        echo -e "Could rollback to previous release: ${PREVIOUS_RELEASE} using command:"
        echo -e "helm rollback ${RELEASE_NAME} ${PREVIOUS_RELEASE}"
        # helm rollback ${RELEASE_NAME} ${PREVIOUS_RELEASE}
        # echo -e "History for release:${RELEASE_NAME}"
        # helm history ${RELEASE_NAME}
        # echo "Deployed Services:"
        # kubectl describe services ${RELEASE_NAME}-${CHART_NAME} --namespace ${CLUSTER_NAMESPACE}
        # echo ""
        # echo "Deployed Pods:"
        # kubectl describe pods --selector app=${CHART_NAME} --namespace ${CLUSTER_NAMESPACE}
        exit 1
      fi

      echo ""
      echo "=========================================================="
      echo "DEPLOYMENTS:"
      echo ""
      echo -e "Status for release:${RELEASE_NAME}"
      helm status ${RELEASE_NAME}

      echo ""
      echo -e "History for release:${RELEASE_NAME}"
      helm history ${RELEASE_NAME}

      echo "=========================================================="
      APP_NAME=$(kubectl get pods --namespace ${CLUSTER_NAMESPACE} -o json | jq -r '[ .items[] | select(.spec.containers[]?.image=="'"${IMAGE_REPOSITORY}:${IMAGE_TAG}"'") | .metadata.labels.app] [1]')
      echo -e "APP: ${APP_NAME}"
      echo "DEPLOYED PODS:"
      kubectl describe pods --selector app=${APP_NAME} --namespace ${CLUSTER_NAMESPACE}

      # lookup service for current release
      APP_SERVICE=$(kubectl get services --namespace ${CLUSTER_NAMESPACE} -o json | jq -r ' .items[] | select (.spec.selector.release=="'"${RELEASE_NAME}"'") | .metadata.name ')
      if [ -z "${APP_SERVICE}" ]; then
        # lookup service for current app
        APP_SERVICE=$(kubectl get services --namespace ${CLUSTER_NAMESPACE} -o json | jq -r ' .items[] | select (.spec.selector.app=="'"${APP_NAME}"'") | .metadata.name ')
      fi
      if [ ! -z "${APP_SERVICE}" ]; then
        echo -e "SERVICE: ${APP_SERVICE}"
        echo "DEPLOYED SERVICES:"
        kubectl describe services ${APP_SERVICE} --namespace ${CLUSTER_NAMESPACE}
      fi

      echo ""
      echo "=========================================================="
      echo "DEPLOYMENT SUCCEEDED"
      if [ ! -z "${APP_SERVICE}" ]; then
        echo ""
        echo ""
        IP_ADDR=$(ibmcloud cs workers ${PIPELINE_KUBERNETES_CLUSTER_NAME} | grep normal | head -n 1 | awk '{ print $2 }')
        if [ "${USE_ISTIO_GATEWAY}" = true ]; then
          PORT=$( kubectl get svc istio-ingressgateway -n istio-system -o json | jq -r '.spec.ports[] | select (.name=="http2") | .nodePort ' )
          echo -e "*** istio gateway enabled ***"
        else
          PORT=$( kubectl get services --namespace ${CLUSTER_NAMESPACE} | grep ${APP_SERVICE} | sed 's/.*:\([0-9]*\).*/\1/g' )
        fi
        echo -e "VIEW THE APPLICATION AT: http://${IP_ADDR}:${PORT}"
      fi
